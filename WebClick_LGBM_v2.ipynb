{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "1l6CpHgu4QAODpybn64R3yF7IS6JpjeMJ",
      "authorship_tag": "ABX9TyP4o8vxxHQ0VYjsClylcCGT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tiabet/DACON_WebClick/blob/main/WebClick_LGBM_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fok1e2qwVot",
        "outputId": "3336c7ef-048a-4181-9de9-ae3b7a54913e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U lightgbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMyGjaRvy4OI",
        "outputId": "607d841a-8f77-4fd3-80dd-d74f3d8501ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Collecting lightgbm\n",
            "  Downloading lightgbm-4.3.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.11.4)\n",
            "Installing collected packages: lightgbm\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 4.1.0\n",
            "    Uninstalling lightgbm-4.1.0:\n",
            "      Successfully uninstalled lightgbm-4.1.0\n",
            "Successfully installed lightgbm-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VRl5-bRPto7x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_parquet('/content/drive/MyDrive/data/train_100000.parquet')\n",
        "test = pd.read_parquet('/content/drive/MyDrive/data/test_100000.parquet')"
      ],
      "metadata": {
        "id": "XjCwG9lHt3hn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['F01'].value_counts()"
      ],
      "metadata": {
        "id": "3Fqfu2q8uG6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['F01'].value_counts()"
      ],
      "metadata": {
        "id": "gZBOOxQHuwES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 동일한 결과 보장을 위해 Seed값을 고정합니다\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "seed_everything(42) # Seed를 42로 고정"
      ],
      "metadata": {
        "id": "rRE-enomu_VX"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.groupby('Click').apply(lambda x: x.sample(min(len(x), 5569860)))"
      ],
      "metadata": {
        "id": "cK_MjZnsvuLw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train.drop('Click', axis = 1)\n",
        "train_y = train['Click']"
      ],
      "metadata": {
        "id": "Fa7wL4ZpvHqv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train"
      ],
      "metadata": {
        "id": "gxrqB8EM0FRF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encode categorical columns\n",
        "def label_encode_columns(train_df, test_df):\n",
        "    le = LabelEncoder()\n",
        "    for column in train_df.columns:\n",
        "        if train_df[column].dtype == 'object' or isinstance(train_df[column].dtype, pd.CategoricalDtype):\n",
        "            # Fit the LabelEncoder on the combined data to ensure consistency\n",
        "            combined_data = pd.concat([train_df[column], test_df[column]], axis=0).astype(str)\n",
        "            le.fit(combined_data)\n",
        "            train_df[column] = le.transform(train_df[column].astype(str))\n",
        "            test_df[column] = le.transform(test_df[column].astype(str))\n",
        "    return train_df, test_df\n",
        "\n",
        "# Apply label encoding to train_x and test_x\n",
        "train_x, test_x = label_encode_columns(train_x, test)"
      ],
      "metadata": {
        "id": "gFm0eN6szzC7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_x)==len(train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTTCUkdcv9-M",
        "outputId": "e6b31cf6-b197-43d5-97fb-976e84aa5be8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "an-FT49jv-N_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optuna 하이퍼파라미터 튜닝"
      ],
      "metadata": {
        "id": "0MLLHAt6wgFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LGBMClassifier(colsample_bytree=0.6829731847990743,\n",
        "               learning_rate=0.041926565437435385, max_bin=1023,\n",
        "               min_child_samples=14, n_estimators=6159, n_jobs=-1,\n",
        "               num_leaves=225, reg_alpha=0.00706357318094864,\n",
        "               reg_lambda=0.4980866507512539, verbose=-1)"
      ],
      "metadata": {
        "id": "sSxfsPS5w_Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from lightgbm import early_stopping\n",
        "from sklearn import metrics\n",
        "\n",
        "def objective(trial):\n",
        "    param = {\n",
        "        'objective': 'binary',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'n_jobs':-1,\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
        "        'max_bin': 1023,\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 3000, 8000),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 100, 300),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 0.1),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 1.0),\n",
        "    }\n",
        "\n",
        "    clf = lgb.LGBMClassifier(**param)\n",
        "    # Include eval_set and early_stopping_rounds\n",
        "    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[early_stopping(stopping_rounds=10)])\n",
        "    preds = clf.predict_proba(X_val)[:, 1]\n",
        "    auc = metrics.roc_auc_score(y_val, preds)\n",
        "    return auc\n"
      ],
      "metadata": {
        "id": "9IHtgBL_wCUZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "h9u4_f0Uw7fm",
        "outputId": "a3cbd3bf-e0d7-46c7-9b0d-3b33d813a74e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-27 12:25:39,669] A new study created in memory with name: no-name-a3381fe2-a8e4-4431-bfef-4c4f391621b6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1168]\tvalid_0's binary_logloss: 0.611641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-27 12:31:01,293] Trial 0 finished with value: 0.7238284782938716 and parameters: {'colsample_bytree': 0.7542950498068064, 'learning_rate': 0.05980373813541492, 'min_child_samples': 26, 'n_estimators': 6366, 'num_leaves': 138, 'reg_alpha': 0.006934983447654428, 'reg_lambda': 0.7040424711264828}. Best is trial 0 with value: 0.7238284782938716.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "Early stopping, best iteration is:\n",
            "[366]\tvalid_0's binary_logloss: 0.612234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-27 12:33:10,608] Trial 1 finished with value: 0.7230682874533236 and parameters: {'colsample_bytree': 0.534451000344625, 'learning_rate': 0.09573555229240897, 'min_child_samples': 17, 'n_estimators': 7890, 'num_leaves': 243, 'reg_alpha': 0.06924498769351944, 'reg_lambda': 0.44878418658848407}. Best is trial 0 with value: 0.7238284782938716.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "Early stopping, best iteration is:\n",
            "[2206]\tvalid_0's binary_logloss: 0.611513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-27 12:43:14,904] Trial 2 finished with value: 0.724024724775642 and parameters: {'colsample_bytree': 0.7629020005212874, 'learning_rate': 0.030519849738200946, 'min_child_samples': 9, 'n_estimators': 5959, 'num_leaves': 155, 'reg_alpha': 0.06930045775882353, 'reg_lambda': 0.9045824073499228}. Best is trial 2 with value: 0.724024724775642.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1606]\tvalid_0's binary_logloss: 0.611803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-05-27 12:50:15,349] Trial 3 finished with value: 0.7236515128661177 and parameters: {'colsample_bytree': 0.852711648698804, 'learning_rate': 0.04578916459688215, 'min_child_samples': 10, 'n_estimators': 6770, 'num_leaves': 123, 'reg_alpha': 0.049996530538146434, 'reg_lambda': 0.2198878813470332}. Best is trial 2 with value: 0.724024724775642.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2024-05-27 12:51:49,555] Trial 4 failed with parameters: {'colsample_bytree': 0.6024471871948847, 'learning_rate': 0.05097697750794569, 'min_child_samples': 27, 'n_estimators': 4799, 'num_leaves': 261, 'reg_alpha': 0.04049687105507829, 'reg_lambda': 0.15990929535025375} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"<ipython-input-57-c996bde8f01f>\", line 24, in objective\n",
            "    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[early_stopping(stopping_rounds=10)])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\", line 1142, in fit\n",
            "    _y = self._le.transform(y)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\", line 842, in fit\n",
            "    elif isinstance(collection, list):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\", line 276, in train\n",
            "    booster.update(fobj=fobj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 3658, in update\n",
            "    tree: Dict[str, Any],\n",
            "KeyboardInterrupt\n",
            "[W 2024-05-27 12:51:49,557] Trial 4 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-e5765535ee57>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[0;32m--> 451\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     ):\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-c996bde8f01f>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Include eval_set and early_stopping_rounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0m_LGBMCheckClassificationTargets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LGBMLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    840\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    274\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LGBM_BoosterEvalMethodResultType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             def _get_node_index(\n\u001b[0;32m-> 3658\u001b[0;31m                 \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3659\u001b[0m                 \u001b[0mtree_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3660\u001b[0m             ) -> str:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbNcLFsPTd6D",
        "outputId": "9c28ae50-63bc-48d6-a815-bef419f11bcc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flaml\n",
            "  Downloading FLAML-2.1.2-py3-none-any.whl (296 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/296.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: NumPy>=1.17 in /usr/local/lib/python3.10/dist-packages (from flaml) (1.25.2)\n",
            "Installing collected packages: flaml\n",
            "Successfully installed flaml-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flaml import AutoML"
      ],
      "metadata": {
        "id": "uwwmOZnlTeSF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize AutoML\n",
        "automl = AutoML()\n",
        "\n",
        "automl_settings = {\n",
        "    \"time_budget\": 7200,  # Total time budget in seconds\n",
        "    \"metric\": 'roc_auc',  # Evaluation metric\n",
        "    \"task\": 'classification',  # Task type\n",
        "    \"log_file_name\": 'automl.log',  # Log file\n",
        "    \"estimator_list\": ['lgbm', 'xgboost', 'catboost', 'rf', 'extra_tree'],  # List of estimators to use\n",
        "    \"estimator_list\": ['lgbm'],\n",
        "    \"eval_method\": \"holdout\",  # Use holdout validation method\n",
        "    # \"split_ratio\": 0.2,  # Ratio of data to be used as validation set\n",
        "    \"early_stop\": 10\n",
        "}\n"
      ],
      "metadata": {
        "id": "YJWCP3wQTfh0"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "automl.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, **automl_settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrOCqA1CVM4d",
        "outputId": "4d196622-4ee8-47ef-c528-0b26e7435f7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[flaml.automl.logger: 05-27 13:56:02] {1680} INFO - task = classification\n",
            "[flaml.automl.logger: 05-27 13:56:02] {1688} INFO - Data split method: stratified\n",
            "[flaml.automl.logger: 05-27 13:56:02] {1691} INFO - Evaluation method: holdout\n",
            "[flaml.automl.logger: 05-27 13:56:09] {1789} INFO - Minimizing error metric: 1-roc_auc\n",
            "[flaml.automl.logger: 05-27 13:56:09] {1901} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
            "[flaml.automl.logger: 05-27 13:56:09] {2219} INFO - iteration 0, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:10] {2345} INFO - Estimated sufficient time budget=9194750s. Estimated necessary time budget=9195s.\n",
            "[flaml.automl.logger: 05-27 13:56:10] {2392} INFO -  at 89.1s,\testimator lgbm's best error=0.3827,\tbest estimator lgbm's best error=0.3827\n",
            "[flaml.automl.logger: 05-27 13:56:10] {2219} INFO - iteration 1, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:11] {2392} INFO -  at 90.2s,\testimator lgbm's best error=0.3827,\tbest estimator lgbm's best error=0.3827\n",
            "[flaml.automl.logger: 05-27 13:56:11] {2219} INFO - iteration 2, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:12] {2392} INFO -  at 91.3s,\testimator lgbm's best error=0.3739,\tbest estimator lgbm's best error=0.3739\n",
            "[flaml.automl.logger: 05-27 13:56:12] {2219} INFO - iteration 3, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:13] {2392} INFO -  at 92.6s,\testimator lgbm's best error=0.3339,\tbest estimator lgbm's best error=0.3339\n",
            "[flaml.automl.logger: 05-27 13:56:13] {2219} INFO - iteration 4, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:14] {2392} INFO -  at 93.7s,\testimator lgbm's best error=0.3339,\tbest estimator lgbm's best error=0.3339\n",
            "[flaml.automl.logger: 05-27 13:56:14] {2219} INFO - iteration 5, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:16] {2392} INFO -  at 95.0s,\testimator lgbm's best error=0.3339,\tbest estimator lgbm's best error=0.3339\n",
            "[flaml.automl.logger: 05-27 13:56:16] {2219} INFO - iteration 6, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:17] {2392} INFO -  at 96.3s,\testimator lgbm's best error=0.3339,\tbest estimator lgbm's best error=0.3339\n",
            "[flaml.automl.logger: 05-27 13:56:17] {2219} INFO - iteration 7, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:18] {2392} INFO -  at 97.5s,\testimator lgbm's best error=0.3339,\tbest estimator lgbm's best error=0.3339\n",
            "[flaml.automl.logger: 05-27 13:56:18] {2219} INFO - iteration 8, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:20] {2392} INFO -  at 99.5s,\testimator lgbm's best error=0.3297,\tbest estimator lgbm's best error=0.3297\n",
            "[flaml.automl.logger: 05-27 13:56:20] {2219} INFO - iteration 9, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:21] {2392} INFO -  at 100.8s,\testimator lgbm's best error=0.3297,\tbest estimator lgbm's best error=0.3297\n",
            "[flaml.automl.logger: 05-27 13:56:21] {2219} INFO - iteration 10, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:24] {2392} INFO -  at 103.6s,\testimator lgbm's best error=0.3297,\tbest estimator lgbm's best error=0.3297\n",
            "[flaml.automl.logger: 05-27 13:56:24] {2219} INFO - iteration 11, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:26] {2392} INFO -  at 105.2s,\testimator lgbm's best error=0.3297,\tbest estimator lgbm's best error=0.3297\n",
            "[flaml.automl.logger: 05-27 13:56:26] {2219} INFO - iteration 12, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:27] {2392} INFO -  at 106.7s,\testimator lgbm's best error=0.3297,\tbest estimator lgbm's best error=0.3297\n",
            "[flaml.automl.logger: 05-27 13:56:27] {2219} INFO - iteration 13, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:29] {2392} INFO -  at 108.4s,\testimator lgbm's best error=0.3297,\tbest estimator lgbm's best error=0.3297\n",
            "[flaml.automl.logger: 05-27 13:56:29] {2219} INFO - iteration 14, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:31] {2392} INFO -  at 110.3s,\testimator lgbm's best error=0.3137,\tbest estimator lgbm's best error=0.3137\n",
            "[flaml.automl.logger: 05-27 13:56:31] {2219} INFO - iteration 15, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:33] {2392} INFO -  at 112.3s,\testimator lgbm's best error=0.3137,\tbest estimator lgbm's best error=0.3137\n",
            "[flaml.automl.logger: 05-27 13:56:33] {2219} INFO - iteration 16, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:35] {2392} INFO -  at 114.0s,\testimator lgbm's best error=0.3137,\tbest estimator lgbm's best error=0.3137\n",
            "[flaml.automl.logger: 05-27 13:56:35] {2219} INFO - iteration 17, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:36] {2392} INFO -  at 115.7s,\testimator lgbm's best error=0.3126,\tbest estimator lgbm's best error=0.3126\n",
            "[flaml.automl.logger: 05-27 13:56:36] {2219} INFO - iteration 18, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:38] {2392} INFO -  at 117.4s,\testimator lgbm's best error=0.3126,\tbest estimator lgbm's best error=0.3126\n",
            "[flaml.automl.logger: 05-27 13:56:38] {2219} INFO - iteration 19, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:39] {2392} INFO -  at 118.8s,\testimator lgbm's best error=0.3126,\tbest estimator lgbm's best error=0.3126\n",
            "[flaml.automl.logger: 05-27 13:56:39] {2219} INFO - iteration 20, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:42] {2392} INFO -  at 121.0s,\testimator lgbm's best error=0.3088,\tbest estimator lgbm's best error=0.3088\n",
            "[flaml.automl.logger: 05-27 13:56:42] {2219} INFO - iteration 21, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:44] {2392} INFO -  at 123.8s,\testimator lgbm's best error=0.3083,\tbest estimator lgbm's best error=0.3083\n",
            "[flaml.automl.logger: 05-27 13:56:44] {2219} INFO - iteration 22, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:47] {2392} INFO -  at 126.2s,\testimator lgbm's best error=0.3076,\tbest estimator lgbm's best error=0.3076\n",
            "[flaml.automl.logger: 05-27 13:56:47] {2219} INFO - iteration 23, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:49] {2392} INFO -  at 128.6s,\testimator lgbm's best error=0.3076,\tbest estimator lgbm's best error=0.3076\n",
            "[flaml.automl.logger: 05-27 13:56:49] {2219} INFO - iteration 24, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:51] {2392} INFO -  at 130.6s,\testimator lgbm's best error=0.3076,\tbest estimator lgbm's best error=0.3076\n",
            "[flaml.automl.logger: 05-27 13:56:51] {2219} INFO - iteration 25, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:54] {2392} INFO -  at 133.0s,\testimator lgbm's best error=0.3076,\tbest estimator lgbm's best error=0.3076\n",
            "[flaml.automl.logger: 05-27 13:56:54] {2219} INFO - iteration 26, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:56:56] {2392} INFO -  at 135.1s,\testimator lgbm's best error=0.3076,\tbest estimator lgbm's best error=0.3076\n",
            "[flaml.automl.logger: 05-27 13:56:56] {2219} INFO - iteration 27, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:00] {2392} INFO -  at 139.6s,\testimator lgbm's best error=0.3076,\tbest estimator lgbm's best error=0.3076\n",
            "[flaml.automl.logger: 05-27 13:57:00] {2219} INFO - iteration 28, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:04] {2392} INFO -  at 143.1s,\testimator lgbm's best error=0.2977,\tbest estimator lgbm's best error=0.2977\n",
            "[flaml.automl.logger: 05-27 13:57:04] {2219} INFO - iteration 29, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:07] {2392} INFO -  at 146.5s,\testimator lgbm's best error=0.2977,\tbest estimator lgbm's best error=0.2977\n",
            "[flaml.automl.logger: 05-27 13:57:07] {2219} INFO - iteration 30, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:11] {2392} INFO -  at 149.9s,\testimator lgbm's best error=0.2977,\tbest estimator lgbm's best error=0.2977\n",
            "[flaml.automl.logger: 05-27 13:57:11] {2219} INFO - iteration 31, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:13] {2392} INFO -  at 152.7s,\testimator lgbm's best error=0.2977,\tbest estimator lgbm's best error=0.2977\n",
            "[flaml.automl.logger: 05-27 13:57:13] {2219} INFO - iteration 32, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:17] {2392} INFO -  at 156.6s,\testimator lgbm's best error=0.2977,\tbest estimator lgbm's best error=0.2977\n",
            "[flaml.automl.logger: 05-27 13:57:17] {2219} INFO - iteration 33, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:20] {2392} INFO -  at 159.0s,\testimator lgbm's best error=0.2977,\tbest estimator lgbm's best error=0.2977\n",
            "[flaml.automl.logger: 05-27 13:57:20] {2219} INFO - iteration 34, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:24] {2392} INFO -  at 163.2s,\testimator lgbm's best error=0.2960,\tbest estimator lgbm's best error=0.2960\n",
            "[flaml.automl.logger: 05-27 13:57:24] {2219} INFO - iteration 35, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:33] {2392} INFO -  at 172.6s,\testimator lgbm's best error=0.2891,\tbest estimator lgbm's best error=0.2891\n",
            "[flaml.automl.logger: 05-27 13:57:33] {2219} INFO - iteration 36, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:39] {2392} INFO -  at 178.3s,\testimator lgbm's best error=0.2891,\tbest estimator lgbm's best error=0.2891\n",
            "[flaml.automl.logger: 05-27 13:57:39] {2219} INFO - iteration 37, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:57:59] {2392} INFO -  at 198.5s,\testimator lgbm's best error=0.2871,\tbest estimator lgbm's best error=0.2871\n",
            "[flaml.automl.logger: 05-27 13:57:59] {2219} INFO - iteration 38, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:58:11] {2392} INFO -  at 210.6s,\testimator lgbm's best error=0.2871,\tbest estimator lgbm's best error=0.2871\n",
            "[flaml.automl.logger: 05-27 13:58:11] {2219} INFO - iteration 39, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 13:59:23] {2392} INFO -  at 282.2s,\testimator lgbm's best error=0.2871,\tbest estimator lgbm's best error=0.2871\n",
            "[flaml.automl.logger: 05-27 13:59:23] {2219} INFO - iteration 40, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 14:02:05] {2392} INFO -  at 444.9s,\testimator lgbm's best error=0.2794,\tbest estimator lgbm's best error=0.2794\n",
            "[flaml.automl.logger: 05-27 14:02:05] {2219} INFO - iteration 41, current learner lgbm\n",
            "[flaml.automl.logger: 05-27 14:02:49] {2392} INFO -  at 488.7s,\testimator lgbm's best error=0.2794,\tbest estimator lgbm's best error=0.2794\n",
            "[flaml.automl.logger: 05-27 14:02:49] {2219} INFO - iteration 42, current learner lgbm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv('drive/MyDrive/data/sample_submission.csv')\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_pred_test_proba = automl.predict_proba(test_x)[:, 1]\n",
        "\n",
        "# Output the probabilities for test set\n",
        "print(y_pred_test_proba)"
      ],
      "metadata": {
        "id": "3ppjODxUVPYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission['Click'] = y_pred_test_proba\n",
        "sample_submission"
      ],
      "metadata": {
        "id": "bVX1ul50VbEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.to_csv(\"drive/MyDrive/data/automl_lgbm_prediction_v2.csv\",index = False)"
      ],
      "metadata": {
        "id": "lnHlWdDMVbR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQqGeJjJVz0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}